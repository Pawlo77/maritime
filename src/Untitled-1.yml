
  #########
  # HBASE #
  #########

  # # Zookeeper (needed for HBase internal coordination)
  # zookeeper:
  #   image: zookeeper:3.7
  #   container_name: zookeeper
  #   ports:
  #     - "2181:2181"

  # # HBase (Master + RegionServer)
  # hbase-master:
  #   image: harisekhon/hbase:2.4
  #   container_name: hbase-master
  #   depends_on:
  #     - namenode
  #     - datanode
  #     - zookeeper
  #   environment:
  #     - HBASE_MANAGES_ZK=false
  #     - HBASE_ZOOKEEPER_QUORUM=zookeeper
  #     - HBASE_ZOOKEEPER_CLIENT_PORT=2181
  #     # Important: configure master-based registry for clients
  #     - HBASE_CLIENT_REGISTRY_IMPL=org.apache.hadoop.hbase.client.MasterRegistry
  #   ports:
  #     - "16010:16010"  # Web UI for HBase Master
  #     - "16000:16000"  # RPC port (if needed)

  # hbase-regionserver:
  #   image: harisekhon/hbase:2.4
  #   container_name: hbase-regionserver
  #   depends_on:
  #     - hbase-master
  #     - namenode
  #     - datanode
  #     - zookeeper
  #   environment:
  #     - HBASE_MANAGES_ZK=false
  #     - HBASE_ZOOKEEPER_QUORUM=zookeeper
  #     - HBASE_ZOOKEEPER_CLIENT_PORT=2181
# x-airflow-common:
#   &airflow-common
#   # In order to add custom dependencies or upgrade provider distributions you can use your extended image.
#   # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml
#   # and uncomment the "build" line below, Then run `docker-compose build` to build the images.
#   image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:3.1.3}
#   # build: .
#   env_file:
#     - ${ENV_FILE_PATH:-.env}
#   environment:
#     &airflow-common-env
#     AIRFLOW__CORE__EXECUTOR: LocalExecutor
#     AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
#     AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
#     AIRFLOW__CORE__FERNET_KEY: ''
#     AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
#     AIRFLOW__CORE__LOAD_EXAMPLES: 'true'
#     AIRFLOW__CORE__EXECUTION_API_SERVER_URL: 'http://airflow-apiserver:8080/execution/'
#     # yamllint disable rule:line-length
#     # Use simple http server on scheduler for health checks
#     # See https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/check-health.html#scheduler-health-check-server
#     # yamllint enable rule:line-length
#     AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
#     # WARNING: Use _PIP_ADDITIONAL_REQUIREMENTS option ONLY for a quick checks
#     # for other purpose (development, test and especially production usage) build/extend Airflow image.
#     _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
#     # The following line can be used to set a custom config file, stored in the local config folder
#     AIRFLOW_CONFIG: '/opt/airflow/config/airflow.cfg'
#   volumes:
#     - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags
#     - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
#     - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config
#     - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins
#   user: "${AIRFLOW_UID:-50000}:0"
#   depends_on:
#     &airflow-common-depends-on
#     postgres:
#       condition: service_healthy


# TODO
# PONIZEJ DZIAÅA
# TRZEBA DODAC HDFS, HBASE i SPARK

# services:

#   #########
#   # Kafka #
#   #########

#   kafka:
#     image: confluentinc/cp-kafka:latest
#     container_name: kafka
#     hostname: kafka
#     ports:
#       - "9092:9092"
#     deploy:
#       resources:
#         limits:
#           cpus: "0.50"
#           memory: 1024m
#     healthcheck:
#       test: ["CMD", "bash", "-c", "echo > /dev/tcp/localhost/9092"]
#       interval: 10s
#       timeout: 5s
#       retries: 5
#       start_period: 15s
#     environment:
#       KAFKA_NODE_ID: 1
#       KAFKA_PROCESS_ROLES: "broker,controller"
#       KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka:29093"
#       KAFKA_LISTENERS: >
#         PLAINTEXT://kafka:29092,
#         CONTROLLER://kafka:29093,
#         PLAINTEXT_HOST://0.0.0.0:9092
#       KAFKA_ADVERTISED_LISTENERS: >
#         PLAINTEXT://kafka:29092,
#         PLAINTEXT_HOST://localhost:9092
#       KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: >
#         PLAINTEXT:PLAINTEXT,
#         PLAINTEXT_HOST:PLAINTEXT,
#         CONTROLLER:PLAINTEXT
#       KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
#       KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
#       KAFKA_LOG_DIRS: /var/lib/kafka/data
#       CLUSTER_ID: MkU3OEVBNTcwNTJENDM2Qk
#       KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
#       KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
#       KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
#       KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
#       KAFKA_IGNORE_FORMATTED: true
#     volumes:
#       - kafka-data:/var/lib/kafka/data
#     restart: unless-stopped

#   kafka-ui:
#     image: provectuslabs/kafka-ui:latest
#     container_name: kafka-ui
#     ports:
#       - "9090:8080"
#     depends_on:
#       kafka:
#         condition: service_healthy
#     deploy:
#       resources:
#         limits:
#           cpus: "0.25"
#           memory: 512m
#     healthcheck:
#       test: ["CMD", "curl", "-f", "http://localhost:9090/actuator/health"]
#       interval: 10s
#       timeout: 5s
#       retries: 5
#       start_period: 180s
#     environment:
#       KAFKA_CLUSTERS_0_NAME: "local"
#       KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: "kafka:29092"
#       KAFKA_CLUSTERS_0_ZOOKEEPER: ""
#     restart: unless-stopped

#   kafka-topic-setup:
#     image: confluentinc/cp-kafka:latest
#     container_name: kafka-topic-setup
#     depends_on:
#       kafka:
#         condition: service_healthy
#     deploy:
#       resources:
#         limits:
#           cpus: "0.25"
#           memory: 256m
#     entrypoint: ["/scripts/create-topics.sh"]
#     volumes:
#       - ./setup/create-topics.sh:/scripts/create-topics.sh
#     restart: "no"

#   ais-producer:
#     build:
#       context: ./ais-kafka-producer
#     container_name: ais-producer
#     depends_on:
#       kafka-topic-setup:
#         condition: service_completed_successfully
#     restart: "on-failure"
#     deploy:
#       resources:
#         limits:
#           cpus: "0.25"
#           memory: 256m

#   ###########
#   # Airflow #
#   ###########

#   postgres:
#     container_name: postgres
#     image: postgres:16
#     ports:
#       - "5432:5432"
#     deploy:
#       resources:
#         limits:
#           cpus: "0.50"
#           memory: 512m
#     environment:
#       POSTGRES_USER: airflow
#       POSTGRES_PASSWORD: airflow
#       POSTGRES_DB: airflow
#     volumes:
#       - postgres-db-data:/var/lib/postgresql/data
#     healthcheck:
#       test: ["CMD", "pg_isready", "-U", "airflow"]
#       interval: 10s
#       retries: 5
#       start_period: 30s
#     restart: always

#   airflow-setup:
#     <<: *airflow-common
#     container_name: airflow-setup
#     deploy:
#       resources:
#         limits:
#           cpus: "0.25"
#           memory: 256m
#     entrypoint: /bin/bash
#     command:
#       - -c
#       - |
#         if [[ -z "${AIRFLOW_UID}" ]]; then
#           echo
#           echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
#           echo "If you are on Linux, you SHOULD follow the instructions below to set "
#           echo "AIRFLOW_UID environment variable, otherwise files will be owned by root."
#           echo "For other operating systems you can get rid of the warning with manually created .env file:"
#           echo "    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user"
#           echo
#         fi
#         mkdir -p /sources/logs /sources/dags /sources/plugins
#         chown -R "${AIRFLOW_UID}:0" /sources/{logs,dags,plugins}
#         exec /entrypoint airflow version
#     environment:
#       <<: *airflow-common-env
#       _AIRFLOW_DB_MIGRATE: 'true'
#       _AIRFLOW_WWW_USER_CREATE: 'true'
#       _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
#       _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
#     user: "0:0"
#     volumes:
#       - ${AIRFLOW_PROJ_DIR:-.}:/sources
#     restart: "no"

#   airflow-apiserver:
#     <<: *airflow-common
#     container_name: airflow-apiserver
#     command: api-server
#     ports:
#       - "8090:8080"
#     deploy:
#       resources:
#         limits:
#           cpus: "0.50"
#           memory: 1024m
#     healthcheck:
#       test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
#       interval: 30s
#       timeout: 10s
#       retries: 5
#       start_period: 180s
#     restart: always
#     depends_on:
#       <<: *airflow-common-depends-on
#       airflow-setup:
#         condition: service_completed_successfully

#   airflow-scheduler:
#     <<: *airflow-common
#     container_name: airflow-scheduler
#     command: scheduler
#     deploy:
#       resources:
#         limits:
#           cpus: "1.0"
#           memory: 2048m
#     healthcheck:
#       test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
#       interval: 30s
#       timeout: 10s
#       retries: 5
#       start_period: 180s
#     restart: always
#     depends_on:
#       <<: *airflow-common-depends-on
#       airflow-setup:
#         condition: service_completed_successfully

#   airflow-dag-processor:
#     <<: *airflow-common
#     container_name: airflow-dag-processor
#     command: dag-processor
#     deploy:
#       resources:
#         limits:
#           cpus: "0.25"
#           memory: 512m
#     healthcheck:
#       test: ["CMD-SHELL", 'airflow jobs check --job-type DagProcessorJob --hostname "$${HOSTNAME}"']
#       interval: 30s
#       timeout: 10s
#       retries: 5
#       start_period: 30s
#     restart: always
#     depends_on:
#       <<: *airflow-common-depends-on
#       airflow-setup:
#         condition: service_completed_successfully

#   airflow-triggerer:
#     <<: *airflow-common
#     container_name: airflow-triggerer
#     deploy:
#       resources:
#         limits:
#           cpus: "0.25"
#           memory: 512m
#     command: triggerer
#     healthcheck:
#       test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
#       interval: 30s
#       timeout: 10s
#       retries: 5
#       start_period: 30s
#     restart: always
#     depends_on:
#       <<: *airflow-common-depends-on
#       airflow-setup:
#         condition: service_completed_successfully

# volumes:
#   kafka-data:
#   postgres-db-data:
